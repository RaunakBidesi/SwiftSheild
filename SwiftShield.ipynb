{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaunakBidesi/SwiftSheild/blob/main/SwiftShield.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF05JmSEKMRl",
        "outputId": "66f12d3b-84cf-414c-c604-9e2e8cd02109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVnlXZXiNMQF",
        "outputId": "e0977f10-55be-40b2-fd5b-d1741cb89fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 150 fight videos and 150 non-fight videos.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Path to your dataset inside Google Drive\n",
        "base_path = \"/content/drive/MyDrive/SwiftSheild\"\n",
        "fight_path = os.path.join(base_path, \"fight\")  # Path to Fight folder\n",
        "non_fight_path = os.path.join(base_path, \"nofight\")  # Path to Non-Fight folder\n",
        "\n",
        "# Get list of video files\n",
        "fight_videos = [os.path.join(fight_path, f) for f in os.listdir(fight_path) if f.endswith(\".mp4\")]\n",
        "non_fight_videos = [os.path.join(non_fight_path, f) for f in os.listdir(non_fight_path) if f.endswith(\".mp4\")]\n",
        "\n",
        "print(f\"Found {len(fight_videos)} fight videos and {len(non_fight_videos)} non-fight videos.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoY1-ljgNd3C",
        "outputId": "56781b3e-3499-4c32-bca0-7858e4cfaf6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame extraction complete!\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Paths for saving extracted frames\n",
        "fight_frames_path = os.path.join(base_path, \"fight_frames\")\n",
        "non_fight_frames_path = os.path.join(base_path, \"non_fight_frames\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(fight_frames_path, exist_ok=True)\n",
        "os.makedirs(non_fight_frames_path, exist_ok=True)\n",
        "\n",
        "# Function to extract frames dynamically based on video length\n",
        "def extract_frames(video_path, output_folder, fps_factor=10, img_size=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open {video_path}\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS)) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30  # Default to 30 FPS if unknown\n",
        "    video_duration = total_frames / fps  # Calculate video length in seconds\n",
        "\n",
        "    # Determine the number of frames to extract dynamically\n",
        "    num_frames = min(int(video_duration * fps_factor), total_frames)  # Extract 10 frames per second\n",
        "\n",
        "    # Get evenly spaced frame indices\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    video_name = os.path.basename(video_path).split('.')[0]  # Get video name without extension\n",
        "\n",
        "    for idx, frame_idx in enumerate(frame_indices):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)  # Move to specific frame\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(f\"Warning: Could not read frame {frame_idx} in {video_path}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        frame = cv2.resize(frame, img_size)  # Resize to 224x224\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "        frame_path = os.path.join(output_folder, f\"{video_name}_frame{idx}.jpg\")\n",
        "        cv2.imwrite(frame_path, frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "# Extract frames for fight videos\n",
        "for video in fight_videos:\n",
        "    extract_frames(video, fight_frames_path, fps_factor=10)\n",
        "\n",
        "# Extract frames for non-fight videos\n",
        "for video in non_fight_videos:\n",
        "    extract_frames(video, non_fight_frames_path, fps_factor=10)\n",
        "\n",
        "print(\"Frame extraction complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51jF5ztmIC1f",
        "outputId": "b6fff34a-37dd-41d2-a4a1-135ac556fc36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fight images: 3374\n",
            "Non-Fight images: 3092\n",
            "✅ No corrupt images found!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "def count_images(folder_path):\n",
        "    return len([f for f in os.listdir(folder_path) if f.endswith('.jpg')])\n",
        "\n",
        "def check_corrupt_images(folder_path):\n",
        "    corrupt_files = []\n",
        "    for img_file in os.listdir(folder_path):\n",
        "        if img_file.endswith('.jpg'):\n",
        "            img_path = os.path.join(folder_path, img_file)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None:\n",
        "                corrupt_files.append(img_path)\n",
        "\n",
        "    return corrupt_files\n",
        "\n",
        "# Count images in each category\n",
        "fight_images_count = count_images(fight_frames_path)\n",
        "non_fight_images_count = count_images(non_fight_frames_path)\n",
        "\n",
        "print(f\"Fight images: {fight_images_count}\")\n",
        "print(f\"Non-Fight images: {non_fight_images_count}\")\n",
        "\n",
        "# Check for corrupt images\n",
        "corrupt_fight = check_corrupt_images(fight_frames_path)\n",
        "corrupt_non_fight = check_corrupt_images(non_fight_frames_path)\n",
        "\n",
        "if corrupt_fight or corrupt_non_fight:\n",
        "    print(\"⚠️ Warning: Found corrupt images!\")\n",
        "    print(f\"Corrupt Fight Images: {len(corrupt_fight)}\")\n",
        "    print(f\"Corrupt Non-Fight Images: {len(corrupt_non_fight)}\")\n",
        "else:\n",
        "    print(\"✅ No corrupt images found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uegomK54LF_v",
        "outputId": "61dff966-3f70-49c9-8023-dc528498fade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Dataset organization complete!\n"
          ]
        }
      ],
      "source": [
        "import os  # OS module to handle file paths and directory operations\n",
        "import shutil  # shutil is used to move files into different folders\n",
        "import random  # Random module to shuffle images before splitting\n",
        "\n",
        "# Define base dataset directory inside Google Drive\n",
        "dataset_base = \"/content/drive/MyDrive/SwiftSheild/dataset\"\n",
        "\n",
        "# Paths where extracted fight & non-fight images are stored\n",
        "fight_frames_path = os.path.join(base_path, \"fight_frames\")\n",
        "non_fight_frames_path = os.path.join(base_path, \"non_fight_frames\")\n",
        "\n",
        "# Define paths for train, validation, and test directories\n",
        "train_dir = os.path.join(dataset_base, \"train\")\n",
        "val_dir = os.path.join(dataset_base, \"val\")\n",
        "test_dir = os.path.join(dataset_base, \"test\")\n",
        "\n",
        "# Create train, val, and test directories with subdirectories for 'fight' and 'non_fight'\n",
        "for split in [train_dir, val_dir, test_dir]:\n",
        "    os.makedirs(os.path.join(split, \"fight\"), exist_ok=True)  # Create 'fight' folder\n",
        "    os.makedirs(os.path.join(split, \"non_fight\"), exist_ok=True)  # Create 'non_fight' folder\n",
        "\n",
        "# Function to split dataset into train, validation, and test sets\n",
        "def split_data(source_folder, train_dest, val_dest, test_dest, train_ratio=0.8, val_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Splits images from the source folder into train, validation, and test folders.\n",
        "\n",
        "    Parameters:\n",
        "        source_folder (str): Path to the folder containing images (fight or non-fight).\n",
        "        train_dest (str): Path to the train folder for this class.\n",
        "        val_dest (str): Path to the validation folder for this class.\n",
        "        test_dest (str): Path to the test folder for this class.\n",
        "        train_ratio (float): Percentage of data to use for training.\n",
        "        val_ratio (float): Percentage of data to use for validation.\n",
        "\n",
        "    The test ratio is automatically calculated as: `1 - (train_ratio + val_ratio)`.\n",
        "    \"\"\"\n",
        "\n",
        "    # List all image files in the source folder\n",
        "    files = [f for f in os.listdir(source_folder) if f.endswith(\".jpg\")]\n",
        "\n",
        "    # Shuffle the images randomly to avoid any bias in the dataset split\n",
        "    random.shuffle(files)\n",
        "\n",
        "    # Compute split indices\n",
        "    train_split = int(len(files) * train_ratio)  # First 80% of images for training\n",
        "    val_split = int(len(files) * (train_ratio + val_ratio))  # Next 10% for validation\n",
        "\n",
        "    # Iterate through all image files and move them to appropriate folders\n",
        "    for i, file in enumerate(files):\n",
        "        src_path = os.path.join(source_folder, file)  # Full path of the current image\n",
        "\n",
        "        if i < train_split:  # First 80% go to the train folder\n",
        "            dest_folder = train_dest\n",
        "        elif i < val_split:  # Next 10% go to the validation folder\n",
        "            dest_folder = val_dest\n",
        "        else:  # Remaining 10% go to the test folder\n",
        "            dest_folder = test_dest\n",
        "\n",
        "        # Move the image file to its new location\n",
        "        shutil.move(src_path, os.path.join(dest_folder, file))\n",
        "\n",
        "# Apply the split function to both fight and non-fight image folders\n",
        "split_data(fight_frames_path, os.path.join(train_dir, \"fight\"), os.path.join(val_dir, \"fight\"), os.path.join(test_dir, \"fight\"))\n",
        "split_data(non_fight_frames_path, os.path.join(train_dir, \"non_fight\"), os.path.join(val_dir, \"non_fight\"), os.path.join(test_dir, \"non_fight\"))\n",
        "\n",
        "print(\"✅ Dataset organization complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGgIGRDCMJDT",
        "outputId": "1d2eb70d-3796-41d3-edfb-101a19919e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5172 files belonging to 2 classes.\n",
            "Found 646 files belonging to 2 classes.\n",
            "Found 648 files belonging to 2 classes.\n",
            "✅ Dataset successfully loaded and normalized!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define dataset directory\n",
        "dataset_base = \"/content/drive/MyDrive/SwiftSheild/dataset\"\n",
        "\n",
        "# Load training dataset\n",
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=os.path.join(dataset_base, \"train\"),\n",
        "    image_size=(224, 224),  # Resize to match ResNet50 input\n",
        "    batch_size=32,  # Adjust batch size as needed\n",
        "    shuffle=True  # Shuffle data for better training\n",
        ")\n",
        "\n",
        "# Load validation dataset\n",
        "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=os.path.join(dataset_base, \"val\"),\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load test dataset (used for final evaluation)\n",
        "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=os.path.join(dataset_base, \"test\"),\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    shuffle=False  # No need to shuffle test data\n",
        ")\n",
        "\n",
        "# Normalize pixel values (ResNet expects input in 0-1 range)\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "\n",
        "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "print(\"✅ Dataset successfully loaded and normalized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32B6l2rhMRUL",
        "outputId": "6d7f374b-fd24-47f1-bde2-7c22274c9782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch shape: (32, 224, 224, 3), Labels shape: (32,)\n"
          ]
        }
      ],
      "source": [
        "for images, labels in train_dataset.take(1):\n",
        "    print(f\"Batch shape: {images.shape}, Labels shape: {labels.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "Y562CtueTdZe",
        "outputId": "cc50f75d-4455-4d24-8cc4-a9d4c3d0c2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)          │      \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m262,272\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">262,272</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,850,113\u001b[0m (90.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,850,113</span> (90.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m262,401\u001b[0m (1.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,401</span> (1.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load pre-trained ResNet50 without the classification head (top layers)\n",
        "base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers (don't update weights during initial training)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create the classification head\n",
        "model = models.Sequential([\n",
        "    base_model,  # Pre-trained ResNet50 base\n",
        "    layers.GlobalAveragePooling2D(),  # Converts feature maps to a single vector\n",
        "    layers.Dense(128, activation=\"relu\"),  # Fully connected layer\n",
        "    layers.Dropout(0.5),  # Dropout to prevent overfitting\n",
        "    layers.Dense(1, activation=\"sigmoid\")  # Final layer (binary classification)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Lower LR for stability\n",
        "    loss=\"binary_crossentropy\",  # Binary classification\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEFOKtM2VK4y",
        "outputId": "0b929596-212d-4ab4-da18-4add9c04eacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1409s\u001b[0m 9s/step - accuracy: 0.5065 - loss: 0.7659 - val_accuracy: 0.5728 - val_loss: 0.6852\n",
            "Epoch 2/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1041s\u001b[0m 6s/step - accuracy: 0.5373 - loss: 0.6904 - val_accuracy: 0.5418 - val_loss: 0.6783\n",
            "Epoch 3/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1044s\u001b[0m 6s/step - accuracy: 0.5547 - loss: 0.6843 - val_accuracy: 0.5975 - val_loss: 0.6727\n",
            "Epoch 4/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1032s\u001b[0m 6s/step - accuracy: 0.6056 - loss: 0.6701 - val_accuracy: 0.5898 - val_loss: 0.6693\n",
            "Epoch 5/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1036s\u001b[0m 6s/step - accuracy: 0.6103 - loss: 0.6701 - val_accuracy: 0.6223 - val_loss: 0.6646\n",
            "Epoch 6/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m999s\u001b[0m 6s/step - accuracy: 0.6083 - loss: 0.6642 - val_accuracy: 0.6068 - val_loss: 0.6637\n",
            "Epoch 7/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1026s\u001b[0m 6s/step - accuracy: 0.6153 - loss: 0.6614 - val_accuracy: 0.6734 - val_loss: 0.6558\n",
            "Epoch 8/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m997s\u001b[0m 6s/step - accuracy: 0.6331 - loss: 0.6552 - val_accuracy: 0.6378 - val_loss: 0.6533\n",
            "Epoch 9/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1030s\u001b[0m 6s/step - accuracy: 0.6342 - loss: 0.6512 - val_accuracy: 0.6811 - val_loss: 0.6474\n",
            "Epoch 10/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m999s\u001b[0m 6s/step - accuracy: 0.6396 - loss: 0.6492 - val_accuracy: 0.6904 - val_loss: 0.6432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved at: /content/drive/MyDrive/SwiftSheild/resnet50_fight_detection_initial.h5\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,  # Train for 10 epochs first\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Save the model in Google Drive to prevent losing it again\n",
        "model_save_path = \"/content/drive/MyDrive/SwiftSheild/resnet50_fight_detection_initial.h5\"\n",
        "model.save(model_save_path)\n",
        "print(f\"✅ Model saved at: {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load the saved model\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/SwiftSheild/resnet50_fight_detection_initial.h5\")\n",
        "\n",
        "# Recompile the model to ensure evaluation works\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Now, evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"✅ Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMRgNje4q2e7",
        "outputId": "bb5db7d9-a4ef-4eff-ea6e-9d899c772c9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 6s/step - accuracy: 0.7778 - loss: 0.6004\n",
            "✅ Test Accuracy: 0.6867, Test Loss: 0.6437\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get predictions\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in test_dataset:\n",
        "    preds = model.predict(images)  # Get predicted probabilities\n",
        "    y_true.extend(labels.numpy())  # Store true labels\n",
        "    y_pred.extend(np.round(preds).flatten())  # Convert probabilities to 0 or 1\n",
        "\n",
        "# Convert to numpy arrays for analysis\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Print results\n",
        "print(f\"True Positives (TP): {tp}\")   # Correctly predicted fights\n",
        "print(f\"False Positives (FP): {fp}\")  # Mistakenly predicted fights (false alarms)\n",
        "print(f\"False Negatives (FN): {fn}\")  # Missed fights (dangerous)\n",
        "print(f\"True Negatives (TN): {tn}\")   # Correctly predicted non-fights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbMwsAAfhTeR",
        "outputId": "15dd8e4e-c254-494a-a8f5-bf6c9428508b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "True Positives (TP): 169\n",
            "False Positives (FP): 62\n",
            "False Negatives (FN): 141\n",
            "True Negatives (TN): 276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze only the last 20 layers of ResNet50\n",
        "for layer in base_model.layers[-20:]:  # Adjust as needed\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile with a lower learning rate for fine-tuning\n",
        "fine_tune_lr = 0.00001  # 10x smaller than the original learning rate\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=fine_tune_lr),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Fine-tune for 10 more epochs\n",
        "history_fine = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=10,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save(\"/content/drive/MyDrive/SwiftSheild/resnet50_fight_detection_finetuned.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHB_9OojwGBy",
        "outputId": "388c5ce3-6685-4d9a-96e4-46ee69ee986b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2574s\u001b[0m 16s/step - accuracy: 0.6203 - loss: 0.6493 - val_accuracy: 0.4830 - val_loss: 0.6948\n",
            "Epoch 2/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1351s\u001b[0m 8s/step - accuracy: 0.8880 - loss: 0.3301 - val_accuracy: 0.5124 - val_loss: 0.6726\n",
            "Epoch 3/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1306s\u001b[0m 8s/step - accuracy: 0.9649 - loss: 0.1517 - val_accuracy: 0.9861 - val_loss: 0.1412\n",
            "Epoch 4/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1287s\u001b[0m 8s/step - accuracy: 0.9907 - loss: 0.0687 - val_accuracy: 0.9783 - val_loss: 0.0771\n",
            "Epoch 5/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1313s\u001b[0m 8s/step - accuracy: 0.9913 - loss: 0.0440 - val_accuracy: 0.9706 - val_loss: 0.0869\n",
            "Epoch 6/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1312s\u001b[0m 8s/step - accuracy: 0.9931 - loss: 0.0301 - val_accuracy: 0.9907 - val_loss: 0.0168\n",
            "Epoch 7/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1362s\u001b[0m 8s/step - accuracy: 0.9920 - loss: 0.0243 - val_accuracy: 0.9985 - val_loss: 0.0159\n",
            "Epoch 8/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1274s\u001b[0m 8s/step - accuracy: 0.9970 - loss: 0.0163 - val_accuracy: 0.9985 - val_loss: 0.0078\n",
            "Epoch 9/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1305s\u001b[0m 8s/step - accuracy: 0.9971 - loss: 0.0112 - val_accuracy: 1.0000 - val_loss: 0.0063\n",
            "Epoch 10/10\n",
            "\u001b[1m162/162\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1271s\u001b[0m 8s/step - accuracy: 0.9968 - loss: 0.0116 - val_accuracy: 0.9938 - val_loss: 0.0148\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Define dataset directory\n",
        "dataset_base = \"/content/drive/MyDrive/SwiftSheild/dataset\"\n",
        "\n",
        "# Reload test dataset\n",
        "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=os.path.join(dataset_base, \"test\"),\n",
        "    image_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    shuffle=False  # No need to shuffle test data\n",
        ")\n",
        "\n",
        "# Normalize pixel values (0-1 range)\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "print(\"✅ Test dataset successfully reloaded and normalized!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKZl8-Z-xTaI",
        "outputId": "ed325334-2e73-454c-c5f6-de67bff56620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 648 files belonging to 2 classes.\n",
            "✅ Test dataset successfully reloaded and normalized!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned model\n",
        "model_path = \"/content/drive/MyDrive/SwiftSheild/resnet50_fight_detection_finetuned.h5\"\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "print(f\"✅ Model successfully loaded from: {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0p9xrg6rq0l",
        "outputId": "30cc4cbf-edb3-4485-9422-8eb1961ad3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model successfully loaded from: /content/drive/MyDrive/SwiftSheild/resnet50_fight_detection_finetuned.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recompile model before evaluating (needed after loading from .h5)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),  # Keep the fine-tuned learning rate\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Evaluate the model on test dataset\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f\"✅ Test Accuracy: {test_accuracy:.4f}, Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vNoYcQqx0T2",
        "outputId": "cfc5ea9e-c8fb-49a0-9345-7c0d81bb1b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 5s/step - accuracy: 0.9972 - loss: 0.0083\n",
            "✅ Test Accuracy: 0.9969, Test Loss: 0.0086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_video_path = \"/content/drive/MyDrive/SwiftSheild/hooligan.violance.mp4\"\n"
      ],
      "metadata": {
        "id": "xolzGE_gzjDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Function to extract frames from a single test video\n",
        "def extract_test_video_frames(video_path, fps_factor=10, img_size=(224, 224)):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open {video_path}\")\n",
        "        return None\n",
        "\n",
        "    # Get video properties\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS)) if cap.get(cv2.CAP_PROP_FPS) > 0 else 30  # Default to 30 FPS if unknown\n",
        "    video_duration = total_frames / fps  # Calculate video length in seconds\n",
        "\n",
        "    # Determine the number of frames to extract dynamically\n",
        "    num_frames = min(int(video_duration * fps_factor), total_frames)  # Extract 10 frames per second\n",
        "\n",
        "    # Get evenly spaced frame indices\n",
        "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "\n",
        "    frame_list = []  # List to store extracted frames\n",
        "\n",
        "    for frame_idx in frame_indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)  # Move to specific frame\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            print(f\"Warning: Could not read frame {frame_idx} in {video_path}. Skipping...\")\n",
        "            continue\n",
        "\n",
        "        frame = cv2.resize(frame, img_size)  # Resize to 224x224\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "        frame = frame / 255.0  # Normalize pixel values\n",
        "        frame_list.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Return frames as a NumPy array for prediction\n",
        "    return np.array(frame_list)\n",
        "\n",
        "# Define the test video path\n",
        "test_video_path = \"/content/drive/MyDrive/SwiftSheild/hooligan.violance.mp4\"\n",
        "\n",
        "# Extract frames from the test video\n",
        "frames = extract_test_video_frames(test_video_path)\n",
        "\n",
        "if frames is not None:\n",
        "    print(f\"✅ Extracted {frames.shape[0]} frames from the test video!\")\n",
        "else:\n",
        "    print(\"⚠️ Frame extraction failed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP31_pUFzoRW",
        "outputId": "aadf0d0d-4a8e-4759-c2be-0327e04c5bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extracted 52 frames from the test video!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "# Load the fine-tuned model\n",
        "model_path = \"/content/drive/MyDrive/SwiftSheild/resnet50_fight_detection_finetuned.h5\"\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "print(f\"✅ Model successfully loaded from: {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qYycUWQI63U",
        "outputId": "0b11bac2-4b54-48d9-9bde-7cc567caf95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model successfully loaded from: /content/drive/MyDrive/SwiftSheild/resnet50_fight_detection_finetuned.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.predict(frames)  # Predict fight probabilities for each frame\n",
        "predicted_labels = (predictions > 0.5).astype(int)  # Convert probabilities to 0 (Non-Fight) or 1 (Fight)\n",
        "\n",
        "# Count fight vs. non-fight frames\n",
        "fight_frames = np.sum(predicted_labels)\n",
        "non_fight_frames = len(predicted_labels) - fight_frames\n",
        "\n",
        "print(f\"✅ Fight Frames Detected: {fight_frames}\")\n",
        "print(f\"✅ Non-Fight Frames Detected: {non_fight_frames}\")\n",
        "\n",
        "# Final decision: Is the video a fight?\n",
        "if fight_frames > non_fight_frames:\n",
        "    print(\"🔥 Fight Detected in the Video! 🔥\")\n",
        "else:\n",
        "    print(\"✅ No Fight Detected in the Video.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Nuq43fdJNwZ",
        "outputId": "cf5c32a7-b323-482e-8a19-68321b117381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 6s/step\n",
            "✅ Fight Frames Detected: 50\n",
            "✅ Non-Fight Frames Detected: 2\n",
            "🔥 Fight Detected in the Video! 🔥\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWs8f64vh2AYgRYYwOGFQ0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}